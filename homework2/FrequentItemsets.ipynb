{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e89a4188",
   "metadata": {},
   "source": [
    "# Homework 2: Discovery of Frequent Itemsets and Association Rules\n",
    "\n",
    "This notebook:\n",
    "1. loads the dataset `data/T10I4D100K.dat` (expected as transactions of item ids),\n",
    "2. implements A-Priori (candidate generation, pruning, counting),\n",
    "3. finds frequent itemsets with support ≥ `min_support` (absolute count),\n",
    "4. prints summary stats and saves results to `frequent_itemsets.csv`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "75cd10c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from collections import defaultdict\n",
    "from itertools import combinations\n",
    "from time import time\n",
    "\n",
    "DATA_PATH = \"data/T10I4D100K.dat\" \n",
    "OUTPUT_CSV = \"frequent_itemsets.csv\"\n",
    "\n",
    "min_support = 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb55e31b",
   "metadata": {},
   "source": [
    "## Load transactions\n",
    "Read the transaction file robustly: each line is a transaction with item ids. The loader tolerates various separators.\n",
    "It returns a list of frozensets (one per transaction), plus counts (n_transactions)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "462722aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 100000 transactions in 0.41s\n"
     ]
    }
   ],
   "source": [
    "def load_transactions(path):\n",
    "    \"\"\"\n",
    "    Load transactions from file path. Each line becomes a transaction.\n",
    "    Items are parsed as tokens of digits (ints). Returns list of frozensets.\n",
    "    \"\"\"\n",
    "    transactions = []\n",
    "    if not os.path.exists(path):\n",
    "        raise FileNotFoundError(f\"{path} not found.\")\n",
    "    with open(path, \"r\") as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "            # split non-digit characters; keep tokens that are digits\n",
    "            # this also handles commas/braces if present\n",
    "            tokens = []\n",
    "            cur = \"\"\n",
    "            for ch in line:\n",
    "                if ch.isdigit():\n",
    "                    cur += ch\n",
    "                else:\n",
    "                    if cur:\n",
    "                        tokens.append(cur)\n",
    "                        cur = \"\"\n",
    "            if cur:\n",
    "                tokens.append(cur)\n",
    "            # convert to ints\n",
    "            try:\n",
    "                items = frozenset(int(t) for t in tokens) if tokens else frozenset()\n",
    "            except ValueError:\n",
    "                # fallback: split on whitespace\n",
    "                parts = [p for p in line.split() if p.strip()]\n",
    "                items = frozenset(int(p) for p in parts)\n",
    "            if items:\n",
    "                transactions.append(items)\n",
    "    return transactions\n",
    "\n",
    "# Load and basic stats\n",
    "t0 = time()\n",
    "transactions = load_transactions(DATA_PATH)\n",
    "n_transactions = len(transactions)\n",
    "t1 = time()\n",
    "print(f\"Loaded {n_transactions} transactions in {t1-t0:.2f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63c1cad4",
   "metadata": {},
   "source": [
    "## Apriori implementation\n",
    "We implement:\n",
    "- `get_frequent_1_itemsets(transactions, min_support)` — count singletons.\n",
    "- `apriori_gen(Lk_minus_1, k)` — candidate generation (join + prune by subsets).\n",
    "- `count_supports(candidates, transactions)` — support counting.\n",
    "- `apriori(transactions, min_support)` — main loop returning dictionary {k: {itemset: support}}.\n",
    "This implementation stores itemsets as tuples with items in sorted order for determinism."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "20ab33be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_frequent_1_itemsets(transactions, min_support):\n",
    "    counts = defaultdict(int)\n",
    "    for t in transactions:\n",
    "        for item in t:\n",
    "            counts[item] += 1\n",
    "    L1 = {}\n",
    "    for item, cnt in counts.items():\n",
    "        if cnt >= min_support:\n",
    "            L1[(item,)] = cnt\n",
    "    return L1\n",
    "\n",
    "def apriori_gen(Lk_minus_1, k):\n",
    "    \"\"\"\n",
    "    Lk_minus_1: dict of (k-1)-itemset tuples -> support\n",
    "    k: target size for candidates\n",
    "    Returns: set of candidate k-itemset tuples (sorted)\n",
    "    \"\"\"\n",
    "    prev_itemsets = sorted(Lk_minus_1.keys())\n",
    "    candidates = set()\n",
    "    len_prev = len(prev_itemsets)\n",
    "    for i in range(len_prev):\n",
    "        for j in range(i+1, len_prev):\n",
    "            a = prev_itemsets[i]\n",
    "            b = prev_itemsets[j]\n",
    "            # join if first k-2 items equal\n",
    "            if a[:k-2] == b[:k-2]:\n",
    "                new_candidate = tuple(sorted(set(a) | set(b)))\n",
    "                if len(new_candidate) == k:\n",
    "                    # prune: all (k-1)-subsets must be in Lk_minus_1\n",
    "                    subsets_ok = True\n",
    "                    for subset in combinations(new_candidate, k-1):\n",
    "                        if tuple(subset) not in Lk_minus_1:\n",
    "                            subsets_ok = False\n",
    "                            break\n",
    "                    if subsets_ok:\n",
    "                        candidates.add(new_candidate)\n",
    "            else:\n",
    "                break  # lexicographic order optimization\n",
    "    return candidates\n",
    "\n",
    "def count_supports(candidates, transactions):\n",
    "    \"\"\"\n",
    "    candidates: iterable of itemset tuples\n",
    "    returns dict itemset->support\n",
    "    \"\"\"\n",
    "    counts = defaultdict(int)\n",
    "    candidate_list = list(candidates)\n",
    "    # for faster lookup convert candidates to set of frozensets\n",
    "    cand_sets = [frozenset(c) for c in candidate_list]\n",
    "    for t in transactions:\n",
    "        # for each candidate, test subset; optimization possible by building subsets of t\n",
    "        for idx, cset in enumerate(cand_sets):\n",
    "            if cset.issubset(t):\n",
    "                counts[candidate_list[idx]] += 1\n",
    "    return dict(counts)\n",
    "\n",
    "def apriori(transactions, min_support, max_k=None):\n",
    "    \"\"\"\n",
    "    Run Apriori, return dict: k -> dict(itemset_tuple -> support)\n",
    "    max_k: optional maximum size of itemsets to produce\n",
    "    \"\"\"\n",
    "    frequent_itemsets = dict()\n",
    "    # L1\n",
    "    L1 = get_frequent_1_itemsets(transactions, min_support)\n",
    "    k = 1\n",
    "    Lk = L1\n",
    "    if Lk:\n",
    "        frequent_itemsets[k] = Lk\n",
    "    while Lk:\n",
    "        k += 1\n",
    "        if max_k and k > max_k:\n",
    "            break\n",
    "        candidates = apriori_gen(Lk, k)\n",
    "        if not candidates:\n",
    "            break\n",
    "        counts = count_supports(candidates, transactions)\n",
    "        # keep those meeting min_support\n",
    "        Lk = {c: cnt for c, cnt in counts.items() if cnt >= min_support}\n",
    "        if Lk:\n",
    "            frequent_itemsets[k] = Lk\n",
    "    return frequent_itemsets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "342ce720",
   "metadata": {},
   "source": [
    "## Run Apriori and show results\n",
    "This cell runs the algorithm with chosen `min_support` and prints:\n",
    "- time taken\n",
    "- count of frequent itemsets per size k\n",
    "- top frequent singletons\n",
    "It also writes results to `frequent_itemsets.csv`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b468e552",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apriori finished in 380.06s\n",
      "Total frequent itemsets found: 385\n",
      " k=1: 375 itemsets\n",
      " k=2: 9 itemsets\n",
      " k=3: 1 itemsets\n",
      "\n",
      "Top frequent singletons (item,), support:\n",
      "(368,) 7828\n",
      "(529,) 7057\n",
      "(829,) 6810\n",
      "(766,) 6265\n",
      "(722,) 5845\n",
      "(354,) 5835\n",
      "(684,) 5408\n",
      "(217,) 5375\n",
      "(494,) 5102\n",
      "(419,) 5057\n",
      "\n",
      "Written frequent itemsets to frequent_itemsets.csv\n"
     ]
    }
   ],
   "source": [
    "t0 = time()\n",
    "freq_itemsets = apriori(transactions, min_support, max_k = 3)\n",
    "t1 = time()\n",
    "print(f\"Apriori finished in {t1-t0:.2f}s\")\n",
    "\n",
    "# summary\n",
    "total_found = sum(len(d) for d in freq_itemsets.values())\n",
    "print(f\"Total frequent itemsets found: {total_found}\")\n",
    "for k in sorted(freq_itemsets):\n",
    "    print(f\" k={k}: {len(freq_itemsets[k])} itemsets\")\n",
    "\n",
    "# top frequent singletons\n",
    "if 1 in freq_itemsets:\n",
    "    top_singletons = sorted(freq_itemsets[1].items(), key=lambda x: -x[1])[:10]\n",
    "    print(\"\\nTop frequent singletons (item,), support:\")\n",
    "    for it, sup in top_singletons:\n",
    "        print(it, sup)\n",
    "else:\n",
    "    print(\"No frequent singletons at this support.\")\n",
    "\n",
    "# Export all frequent itemsets to CSV: columns (k, itemset, support)\n",
    "import csv\n",
    "with open(OUTPUT_CSV, \"w\", newline=\"\") as csvfile:\n",
    "    writer = csv.writer(csvfile)\n",
    "    writer.writerow([\"k\", \"itemset\", \"support\"])\n",
    "    for k in sorted(freq_itemsets):\n",
    "        for itemset, sup in sorted(freq_itemsets[k].items(), key=lambda x:(len(x[0]), -x[1]) if isinstance(x[0], tuple) else (k, -x[1])):\n",
    "            writer.writerow([k, \" \".join(map(str,itemset)), sup])\n",
    "\n",
    "print(f\"\\nWritten frequent itemsets to {OUTPUT_CSV}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0959b851",
   "metadata": {},
   "source": [
    "# Association rule mining (from Apriori frequent itemsets)\n",
    "\n",
    "This section generates association rules A -> B where:\n",
    "- support(A ∪ B) ≥ `min_support` (absolute count),\n",
    "- confidence(A -> B) = support(A ∪ B) / support(A) ≥ `min_confidence`.\n",
    "\n",
    "Rules are produced from the already-computed `freq_itemsets` dictionary:\n",
    "`freq_itemsets[k]` is a dict mapping `tuple` (sorted itemset) → support count.\n",
    "\n",
    "Output: a list of rules with support counts and fractions, confidence and lift; saved to `association_rules.csv`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f802a754",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_transactions: 100000\n",
      "min_support (abs): 1000 min_confidence: 0.6\n"
     ]
    }
   ],
   "source": [
    "# Parameters for rule generation\n",
    "min_support_rules = 1000           # absolute count threshold for support (s)\n",
    "min_confidence = 0.6               # confidence threshold (c) as fraction between 0 and 1\n",
    "top_n_rules_to_print = 30          # how many top rules to show on screen\n",
    "output_rules_csv = \"association_rules.csv\"\n",
    "\n",
    "try:\n",
    "    n_transactions\n",
    "except NameError:\n",
    "    # fallback: compute from transactions if present\n",
    "    try:\n",
    "        n_transactions = len(transactions)\n",
    "    except NameError:\n",
    "        raise RuntimeError(\"n_transactions or transactions not found. Run the data-loading / Apriori cells first.\")\n",
    "print(\"n_transactions:\", n_transactions)\n",
    "print(\"min_support (abs):\", min_support_rules, \"min_confidence:\", min_confidence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95884ce5",
   "metadata": {},
   "source": [
    "## Function: generate association rules\n",
    "\n",
    "This function:\n",
    "- takes `freq_itemsets` (k → {tuple: support_count}),\n",
    "- `min_support` (absolute) and `min_confidence` (fraction),\n",
    "- returns a list of rules as dictionaries with keys:\n",
    "  `antecedent` (tuple), `consequent` (tuple), `support_count`, `support_frac`, `confidence`, `lift`.\n",
    "  \n",
    "Notes:\n",
    "- We require antecedent and consequent to be non-empty and disjoint, and antecedent ∪ consequent to be a frequent itemset.\n",
    "- Lift is computed as: lift = confidence / support(consequent)_fraction = (sup(J)/sup(A)) / (sup(B)/N) = sup(J) * N / (sup(A) * sup(B))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "922ce9e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import combinations\n",
    "from collections import defaultdict\n",
    "import csv\n",
    "\n",
    "def generate_association_rules(freq_itemsets, n_transactions, min_support, min_confidence):\n",
    "    \"\"\"\n",
    "    freq_itemsets: dict k -> {itemset_tuple: support_count}\n",
    "    n_transactions: int\n",
    "    min_support: absolute count threshold (s)\n",
    "    min_confidence: fraction threshold (c)\n",
    "    \n",
    "    returns: list of rule dicts sorted by confidence then support\n",
    "    \"\"\"\n",
    "    # Build support lookup: tuple(sorted(...)) -> count\n",
    "    support = {}\n",
    "    for k, d in freq_itemsets.items():\n",
    "        for itemset, cnt in d.items():\n",
    "            # store as tuple(sorted(...)) for consistency\n",
    "            support[tuple(sorted(itemset))] = cnt\n",
    "\n",
    "    rules = []\n",
    "    # iterate over all frequent itemsets of size >= 2\n",
    "    for k in sorted(freq_itemsets.keys()):\n",
    "        if k < 2:\n",
    "            continue\n",
    "        for itemset_tuple, sup_J in freq_itemsets[k].items():\n",
    "            items = tuple(sorted(itemset_tuple))\n",
    "            # enumerate all non-empty proper antecedent subsets A\n",
    "            n_items = len(items)\n",
    "            # skip if sup_J < min_support (shouldn't happen, but safe)\n",
    "            if sup_J < min_support:\n",
    "                continue\n",
    "            for r in range(1, n_items):\n",
    "                for antecedent in combinations(items, r):\n",
    "                    antecedent = tuple(sorted(antecedent))\n",
    "                    consequent = tuple(sorted(set(items) - set(antecedent)))\n",
    "                    # antecedent must have support (it should, by Apriori) — guard anyway\n",
    "                    sup_A = support.get(antecedent, 0)\n",
    "                    if sup_A == 0:\n",
    "                        continue\n",
    "                    confidence = sup_J / sup_A\n",
    "                    if confidence + 1e-12 >= min_confidence:\n",
    "                        # compute support fraction and lift\n",
    "                        sup_B = support.get(consequent, 0)\n",
    "                        support_frac = sup_J / n_transactions\n",
    "                        # compute lift safely (avoid zero division)\n",
    "                        if sup_B > 0:\n",
    "                            lift = (sup_J * n_transactions) / (sup_A * sup_B)\n",
    "                        else:\n",
    "                            lift = None\n",
    "                        # only include rule if sup_J >= min_support (already true)\n",
    "                        rule = {\n",
    "                            \"antecedent\": antecedent,\n",
    "                            \"consequent\": consequent,\n",
    "                            \"support_count\": sup_J,\n",
    "                            \"support_frac\": support_frac,\n",
    "                            \"confidence\": confidence,\n",
    "                            \"lift\": lift,\n",
    "                            \"support_antecedent\": sup_A,\n",
    "                            \"support_consequent\": sup_B\n",
    "                        }\n",
    "                        rules.append(rule)\n",
    "    # sort rules by confidence desc, then support_count desc\n",
    "    rules_sorted = sorted(rules, key=lambda r: (-r[\"confidence\"], -r[\"support_count\"]))\n",
    "    return rules_sorted\n",
    "\n",
    "# convenience function to write CSV\n",
    "def save_rules_to_csv(rules, path):\n",
    "    with open(path, \"w\", newline=\"\") as csvfile:\n",
    "        writer = csv.writer(csvfile)\n",
    "        writer.writerow([\"antecedent\", \"consequent\", \"support_count\", \"support_frac\", \"confidence\", \"lift\",\n",
    "                        \"support_antecedent\", \"support_consequent\"])\n",
    "        for r in rules:\n",
    "            writer.writerow([\n",
    "                \" \".join(map(str, r[\"antecedent\"])),\n",
    "                \" \".join(map(str, r[\"consequent\"])),\n",
    "                r[\"support_count\"],\n",
    "                f\"{r['support_frac']:.6f}\",\n",
    "                f\"{r['confidence']:.6f}\",\n",
    "                f\"{r['lift']:.6f}\" if r[\"lift\"] is not None else \"\",\n",
    "                r[\"support_antecedent\"],\n",
    "                r[\"support_consequent\"]\n",
    "            ])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0a8a117",
   "metadata": {},
   "source": [
    "## Run rule generation and show top rules\n",
    "This cell runs the generator with the `min_support_rules` and `min_confidence` from the parameters cell,\n",
    "prints the number of rules and the top `top_n_rules_to_print` rules.\n",
    "It also saves rules to `association_rules.csv`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "936fe6da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 5 rules (min_support=1000, min_confidence=0.6)\n",
      "\n",
      "Top 5 rules (antecedent -> consequent) with support_count, support_frac, confidence, lift:\n",
      " 1. 704 825 -> 39    sup=1035 (0.0103)  conf=0.939  lift=22.057\n",
      " 2. 39 704 -> 825    sup=1035 (0.0103)  conf=0.935  lift=30.307\n",
      " 3. 39 825 -> 704    sup=1035 (0.0103)  conf=0.872  lift=48.603\n",
      " 4. 704 -> 39    sup=1107 (0.0111)  conf=0.617  lift=14.492\n",
      " 5. 704 -> 825    sup=1102 (0.0110)  conf=0.614  lift=19.912\n",
      "\n",
      "Saved rules to association_rules.csv\n"
     ]
    }
   ],
   "source": [
    "# generate rules (uses freq_itemsets from Apriori)\n",
    "rules = generate_association_rules(freq_itemsets, n_transactions, min_support_rules, min_confidence)\n",
    "print(f\"Generated {len(rules)} rules (min_support={min_support_rules}, min_confidence={min_confidence})\")\n",
    "\n",
    "# print top rules\n",
    "to_show = min(len(rules), top_n_rules_to_print)\n",
    "if to_show == 0:\n",
    "    print(\"No rules found with those thresholds.\")\n",
    "else:\n",
    "    print(f\"\\nTop {to_show} rules (antecedent -> consequent) with support_count, support_frac, confidence, lift:\")\n",
    "    for i, r in enumerate(rules[:to_show], 1):\n",
    "        ant = \" \".join(map(str, r[\"antecedent\"]))\n",
    "        con = \" \".join(map(str, r[\"consequent\"]))\n",
    "        print(f\"{i:2d}. {ant} -> {con}    sup={r['support_count']} ({r['support_frac']:.4f})  \"\n",
    "            f\"conf={r['confidence']:.3f}  lift={r['lift']:.3f}\" if r['lift'] is not None else\n",
    "            f\"{i:2d}. {ant} -> {con}    sup={r['support_count']} ({r['support_frac']:.4f})  conf={r['confidence']:.3f}\")\n",
    "\n",
    "# save\n",
    "save_rules_to_csv(rules, output_rules_csv)\n",
    "print(f\"\\nSaved rules to {output_rules_csv}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spark-venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
