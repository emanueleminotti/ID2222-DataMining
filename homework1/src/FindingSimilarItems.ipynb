{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6294e2f5",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "6aea195e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import re\n",
    "import time\n",
    "import random\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Custom modules\n",
    "import Shingling\n",
    "import CompareSets\n",
    "import MinHashing\n",
    "import CompareSignatures"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8ca5f9b",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "28fecb72",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = '../data/'\n",
    "\n",
    "# The 'k' in k-shingles\n",
    "SHINGLE_LENGTH = 10\n",
    "\n",
    "# The number of hash functions for MinHashing (signature length 'n')\n",
    "# More hashes = better accuracy, but slower.\n",
    "MINHASH_SIGNATURE_SIZE = 100\n",
    "\n",
    "# The Jaccard similarity threshold 's'\n",
    "SIMILARITY_THRESHOLD = 0.8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20e6c77f",
   "metadata": {},
   "source": [
    "# Load Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "54cf212f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Successfully loaded 10 documents.\n"
     ]
    }
   ],
   "source": [
    "doc_files = sorted(glob.glob(os.path.join(DATA_DIR, '*.txt')))\n",
    "num_docs = len(doc_files)\n",
    "\n",
    "docs = []\n",
    "doc_names = []\n",
    "for doc_path in doc_files:\n",
    "    doc_names.append(os.path.basename(doc_path))\n",
    "    try:\n",
    "        with open(doc_path, 'r', encoding='utf-8') as f:\n",
    "            docs.append(f.read())\n",
    "    except Exception as e:\n",
    "        print(f\"Could not read {doc_path}: {e}\")\n",
    "\n",
    "print(f\"\\nSuccessfully loaded {len(docs)} documents.\")\n",
    "\n",
    "def clean_text(text: str) -> str:\n",
    "    \"\"\"A simple text cleaner.\"\"\"\n",
    "    \n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Remove all punctuation and numbers\n",
    "    text = re.sub(r'[^a-z\\s]', '', text)\n",
    "    \n",
    "    # Compact all whitespace into a single space\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    \n",
    "    return text.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f824687d",
   "metadata": {},
   "source": [
    "# Shingling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "edd3fba5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Stage 1: Shingling (k=10) ---\n",
      "  '041.txt' -> 1098 unique shingles.\n",
      "  '052.txt' -> 752 unique shingles.\n",
      "  '053.txt' -> 824 unique shingles.\n",
      "  '140.txt' -> 2077 unique shingles.\n",
      "  '141.txt' -> 816 unique shingles.\n",
      "  '180.txt' -> 3387 unique shingles.\n",
      "  '207.txt' -> 2112 unique shingles.\n",
      "  '288.txt' -> 1145 unique shingles.\n",
      "  '299.txt' -> 2357 unique shingles.\n",
      "  '438.txt' -> 1278 unique shingles.\n",
      "\n",
      "Shingling all documents took 0.0089 seconds.\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\n--- Stage 1: Shingling (k={SHINGLE_LENGTH}) ---\")\n",
    "start_time = time.time()\n",
    "\n",
    "shingler = Shingling.Shingling(k=SHINGLE_LENGTH)\n",
    "\n",
    "# This list will hold the set of hashed shingles for each document\n",
    "doc_shingle_sets = []\n",
    "\n",
    "for i in range(num_docs):\n",
    "    doc_content = docs[i]\n",
    "    \n",
    "    cleaned_doc = clean_text(doc_content)\n",
    "    hashed_shingles_list = shingler.get_hashed_shingles(cleaned_doc)\n",
    "    shingles_set = set(hashed_shingles_list)\n",
    "    \n",
    "    doc_shingle_sets.append(shingles_set)\n",
    "    print(f\"  '{doc_names[i]}' -> {len(shingles_set)} unique shingles.\")\n",
    "\n",
    "print(f\"\\nShingling all documents took {time.time() - start_time:.4f} seconds.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2999722",
   "metadata": {},
   "source": [
    "# Jaccard Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "e538af5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Stage 2: Jaccard Similarity (Threshold s=0.8) ---\n",
      "\n",
      "Found 0 true similar pairs.\n",
      "Comparison took 0.0103 seconds.\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\n--- Stage 2: Jaccard Similarity (Threshold s={SIMILARITY_THRESHOLD}) ---\")\n",
    "start_time = time.time()\n",
    "\n",
    "ground_truth_similar_pairs = []\n",
    "\n",
    "_ = CompareSets.CompareSets.calculate_jaccard\n",
    "\n",
    "for i in range(num_docs):\n",
    "    for j in range(i + 1, num_docs):\n",
    "        set1 = doc_shingle_sets[i]\n",
    "        set2 = doc_shingle_sets[j]\n",
    "        \n",
    "        j_sim = CompareSets.CompareSets.calculate_jaccard(set1, set2)\n",
    "        \n",
    "        # If similarity is above our threshold, record it\n",
    "        if j_sim >= SIMILARITY_THRESHOLD:\n",
    "            pair = tuple(sorted((doc_names[i], doc_names[j])))\n",
    "            ground_truth_similar_pairs.append((pair, j_sim))\n",
    "            print(f\"  {pair} | Jaccard = {j_sim:.4f}\")\n",
    "\n",
    "print(f\"\\nFound {len(ground_truth_similar_pairs)} true similar pairs.\")\n",
    "print(f\"Comparison took {time.time() - start_time:.4f} seconds.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c0d04ca",
   "metadata": {},
   "source": [
    "# MinHashing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "94d7f4a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Stage 3: Minhashing (Signature Size n=100) ---\n",
      "\n",
      "Minhashing all documents took 0.3364 seconds.\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\n--- Stage 3: Minhashing (Signature Size n={MINHASH_SIGNATURE_SIZE}) ---\")\n",
    "start_time = time.time()\n",
    "\n",
    "# This list will hold the MinHash signatures (vectors) for each document\n",
    "doc_signatures = []\n",
    "\n",
    "minhasher = MinHashing.MinHashing(num_hashes=MINHASH_SIGNATURE_SIZE)\n",
    "\n",
    "for i in range(num_docs):\n",
    "    shingle_set = doc_shingle_sets[i]\n",
    "    \n",
    "    signature = minhasher.get_signature(shingle_set)\n",
    "    doc_signatures.append(signature)\n",
    "\n",
    "print(f\"\\nMinhashing all documents took {time.time() - start_time:.4f} seconds.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe03bef6",
   "metadata": {},
   "source": [
    "# Signature Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "065ec62f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Stage 4: Signature Comparison (Threshold s=0.8) ---\n",
      "\n",
      "Found 0 estimated similar pairs.\n",
      "Signature comparison took 0.0003 seconds.\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\n--- Stage 4: Signature Comparison (Threshold s={SIMILARITY_THRESHOLD}) ---\")\n",
    "start_time = time.time()\n",
    "\n",
    "minhash_similar_pairs = []\n",
    "\n",
    "_ = CompareSignatures.CompareSignatures.calculate_similarity\n",
    "\n",
    "for i in range(num_docs):\n",
    "    for j in range(i + 1, num_docs):\n",
    "        sig1 = doc_signatures[i]\n",
    "        sig2 = doc_signatures[j]\n",
    "        \n",
    "        est_sim = CompareSignatures.CompareSignatures.calculate_similarity(sig1, sig2)\n",
    "        \n",
    "        # If estimated similarity is above our threshold, record it\n",
    "        if est_sim >= SIMILARITY_THRESHOLD:\n",
    "            pair = tuple(sorted((doc_names[i], doc_names[j])))\n",
    "            minhash_similar_pairs.append((pair, est_sim))\n",
    "            print(f\" {pair} | Est. Sim = {est_sim:.4f}\")\n",
    "\n",
    "print(f\"\\nFound {len(minhash_similar_pairs)} estimated similar pairs.\")\n",
    "print(f\"Signature comparison took {time.time() - start_time:.4f} seconds.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef79cd37",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "19887e4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Stage 5: Evaluation ---\n",
      "No similar pairs found by either method.\n",
      "\n",
      "--- Summary ---\n",
      "Ground Truth (Jaccard): 0 pairs\n",
      "MinHash Estimate:       0 pairs\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\n--- Stage 5: Evaluation ---\")\n",
    "\n",
    "true_pairs_set = set([pair[0] for pair in ground_truth_similar_pairs])\n",
    "est_pairs_set = set([pair[0] for pair in minhash_similar_pairs])\n",
    "\n",
    "if not true_pairs_set and not est_pairs_set:\n",
    "    print(\"No similar pairs found by either method.\")\n",
    "else:\n",
    "    true_positives = len(true_pairs_set.intersection(est_pairs_set))\n",
    "    false_positives = len(est_pairs_set.difference(true_pairs_set))\n",
    "    false_negatives = len(true_pairs_set.difference(est_pairs_set))\n",
    "\n",
    "    print(f\"True Positives:   {true_positives}\")\n",
    "    print(f\"False Positives: {false_positives}\")\n",
    "    print(f\"False Negatives: {false_negatives}\")\n",
    "\n",
    "    # Precision and Recall\n",
    "    if (true_positives + false_positives) > 0:\n",
    "        precision = true_positives / (true_positives + false_positives)\n",
    "        print(f\"Precision: {precision:.4f}\")\n",
    "    else:\n",
    "        print(\"Precision: N/A (no positive estimations)\")\n",
    "        \n",
    "    if (true_positives + false_negatives) > 0:\n",
    "        recall = true_positives / (true_positives + false_negatives)\n",
    "        print(f\"Recall:    {recall:.4f}\")\n",
    "    else:\n",
    "        print(\"Recall:    N/A (no true pairs to find)\")\n",
    "\n",
    "print(\"\\n--- Summary ---\")\n",
    "print(f\"Ground Truth (Jaccard): {len(true_pairs_set)} pairs\")\n",
    "print(f\"MinHash Estimate:       {len(est_pairs_set)} pairs\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spark-venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
