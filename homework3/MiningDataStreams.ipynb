{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0c9a4005",
   "metadata": {},
   "source": [
    "# Homework 3: Mining Data Streams\n",
    "\n",
    "This notebook implements and evaluates the **TRIÈST-BASE** and **TRIÈST-IMPR** algorithms as described in the paper \"TRIÈST: Counting Local and Global Triangles in Fully-Dynamic Streams with Fixed Memory Size.\"\n",
    "\n",
    "## Dataset: Stanford Web Graph (`web-Stanford.txt`)\n",
    "\n",
    "The experiment is run on the `web-Stanford` dataset.\n",
    "\n",
    "> Nodes represent pages from Stanford University (stanford.edu) and directed edges represent hyperlinks between them. The data was collected in 2002.\n",
    "\n",
    "**Note:** For this project, the directed edges are treated as undirected to form a graph for triangle counting.\n",
    "\n",
    "### Dataset Statistics\n",
    "\n",
    "| Statistic | Value |\n",
    "| :--- | :--- |\n",
    "| Nodes | 281,903 |\n",
    "| Edges | 2,312,497 |\n",
    "| Average clustering coefficient | 0.5976 |\n",
    "| **Number of triangles (Ground Truth)** | **11,329,473** |\n",
    "| Nodes in largest WCC | 255,265 (0.906) |\n",
    "| Edges in largest WCC | 2,234,572 (0.966) |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2985c4ad",
   "metadata": {},
   "source": [
    "## Conceptual Summary: TRIÈST-BASE\n",
    "\n",
    "**TRIÈST-BASE** is the first algorithm presented in the paper, designed for insertion-only graph streams. Its core idea is to maintain a fixed-size sample of edges, $\\mathcal{S}$, using **Reservoir Sampling** and to only count the triangles that form *within that sample*. This raw count is then scaled up to produce an unbiased estimation of the total triangles in the full graph.\n",
    "\n",
    "### The Core Logic\n",
    "\n",
    "The algorithm's operation at each time step $t$ (when a new edge $(u, v)$ arrives) is divided into two main parts:\n",
    "\n",
    "1.  **Edge Sampling (Reservoir Logic):**\n",
    "    * The algorithm maintains a sample $\\mathcal{S}$ of a fixed size $M$.\n",
    "    * **If $t \\le M$ (Reservoir is filling):** The new edge $(u, v)$ is automatically added to the sample $\\mathcal{S}$.\n",
    "    * **If $t > M$ (Reservoir is full):** The algorithm \"flips a biased coin\" with a probability of $p = M/t$.\n",
    "        * **If the coin is heads (probability $p$):** The new edge $(u, v)$ is kept. To make space, a random edge $(u', v')$ is chosen from the sample $\\mathcal{S}$ and evicted.\n",
    "        * **If the coin is tails (probability $1-p$):** The new edge $(u, v)$ is discarded, and the sample $\\mathcal{S}$ remains unchanged.\n",
    "\n",
    "2.  **Counter Updates (Triangle Counting Logic):**\n",
    "    * This is the most critical part. The algorithm **only** updates its counters when an edge is **inserted into** or **deleted from** the sample $\\mathcal{S}$.\n",
    "    * **When an edge $(u, v)$ is ADDED to $\\mathcal{S}$:**\n",
    "        * The algorithm looks for all common neighbors of $u$ and $v$ that are *already in the sample $\\mathcal{S}$*.\n",
    "        * For every common neighbor $c$ it finds, a new triangle $\\{(u, v), (v, c), (c, u)\\}$ has been formed *in the sample*.\n",
    "        * The global counter, $\\tau$, is **incremented** by the number of common neighbors found.\n",
    "    * **When an edge $(u', v')$ is REMOVED from $\\mathcal{S}$:**\n",
    "        * The algorithm looks for all common neighbors $c'$ of $u'$ and $v'$ that are *currently in the sample $\\mathcal{S}$*.\n",
    "        * For every common neighbor $c'$ it finds, a triangle is being broken *in the sample*.\n",
    "        * The global counter, $\\tau$, is **decremented** by the number of these common neighbors.\n",
    "\n",
    "### The Final Estimation\n",
    "\n",
    "The algorithm does **not** return the raw counter $\\tau$. This counter only represents the number of triangles in the (small) sample $\\mathcal{S}$.\n",
    "\n",
    "To get the final estimation of triangles in the *entire graph*, this count is scaled by a correction factor $\\xi^{(t)}$. This factor represents the ratio of \"possible triangles in the full stream\" to \"possible triangles in the sample.\"\n",
    "\n",
    "* **Final Estimate = $\\tau \\times \\xi^{(t)}$**\n",
    "\n",
    "Where for $t > M$, the scaling factor is:\n",
    "\n",
    "$$\n",
    "\\xi^{(t)} = \\frac{t(t-1)(t-2)}{M(M-1)(M-2)}\n",
    "$$\n",
    "\n",
    "This scaling is what makes the **TRIÈST-BASE** estimation mathematically unbiased."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0c86638",
   "metadata": {},
   "source": [
    "## Imports and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8ea047ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import os\n",
    "import random\n",
    "# Import the algorithms from the local files\n",
    "from src.TriestBase import TriestBase\n",
    "from src.TriestImpr import TriestImpr\n",
    "\n",
    "# Configuration\n",
    "FILE_PATH = 'data/web-Stanford.txt'\n",
    "MEMORY_SIZE_M = 10000  # Fixed memory size M"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ddd05cc",
   "metadata": {},
   "source": [
    "## Stream Processing Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "73ac31af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_stream_and_run(filepath, algo_base, algo_impr, limit=None):\n",
    "    \"\"\"\n",
    "    Reads the file stream and feeds edges to both algorithms simultaneously.\n",
    "    Handles the input as an edge stream (u, v).\n",
    "    \"\"\"\n",
    "    edge_count = 0\n",
    "    start_time = time.time()\n",
    "    \n",
    "    print(f\"Reading stream from {filepath}...\")\n",
    "    \n",
    "    try:\n",
    "        with open(filepath, 'r') as f:\n",
    "            for line in f:\n",
    "                # Skip comments\n",
    "                if line.startswith('#'):\n",
    "                    continue\n",
    "                \n",
    "                parts = line.split()\n",
    "                if len(parts) < 2:\n",
    "                    continue\n",
    "                \n",
    "                try:\n",
    "                    u, v = int(parts[0]), int(parts[1])\n",
    "                except ValueError:\n",
    "                    continue # Skip malformed lines\n",
    "                \n",
    "                # Ignore self-loops as per standard graph stream definitions\n",
    "                if u == v:\n",
    "                    continue\n",
    "                    \n",
    "                # Canonicalize edge (undirected graph assumption for TRIEST)\n",
    "                if u > v:\n",
    "                    u, v = v, u\n",
    "                \n",
    "                # Feed stream to both algorithms\n",
    "                algo_base.process_edge(u, v)\n",
    "                algo_impr.process_edge(u, v)\n",
    "                \n",
    "                edge_count += 1\n",
    "                if edge_count % 100000 == 0:\n",
    "                    print(f\"Processed {edge_count} edges...\")\n",
    "                \n",
    "                if limit and edge_count >= limit:\n",
    "                    break\n",
    "                    \n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: File {filepath} not found.\")\n",
    "        return None\n",
    "        \n",
    "    duration = time.time() - start_time\n",
    "    print(f\"\\n--- Processing Complete in {duration:.2f} seconds ---\")\n",
    "    return edge_count"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d15ef278",
   "metadata": {},
   "source": [
    "## Execution and Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "966b8e86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing Algorithms with Memory M = 10000\n",
      "Reading stream from data/web-Stanford.txt...\n",
      "Processed 100000 edges...\n",
      "Processed 200000 edges...\n",
      "Processed 300000 edges...\n",
      "Processed 400000 edges...\n",
      "Processed 500000 edges...\n",
      "Processed 600000 edges...\n",
      "Processed 700000 edges...\n",
      "Processed 800000 edges...\n",
      "Processed 900000 edges...\n",
      "Processed 1000000 edges...\n",
      "Processed 1100000 edges...\n",
      "Processed 1200000 edges...\n",
      "Processed 1300000 edges...\n",
      "Processed 1400000 edges...\n",
      "Processed 1500000 edges...\n",
      "Processed 1600000 edges...\n",
      "Processed 1700000 edges...\n",
      "Processed 1800000 edges...\n",
      "Processed 1900000 edges...\n",
      "Processed 2000000 edges...\n",
      "Processed 2100000 edges...\n",
      "Processed 2200000 edges...\n",
      "Processed 2300000 edges...\n",
      "\n",
      "--- Processing Complete in 2.93 seconds ---\n",
      "========================================\n",
      "Final Statistics:\n",
      "Total Edges Streamed (t): 2312497\n",
      "Reservoir Size (M):       10000\n",
      "----------------------------------------\n",
      "TRIEST-BASE Estimated Global Triangles: 171734121754\n",
      "TRIEST-IMPR Estimated Global Triangles: 16651610\n"
     ]
    }
   ],
   "source": [
    "print(f\"Initializing Algorithms with Memory M = {MEMORY_SIZE_M}\")\n",
    "t_base = TriestBase(MEMORY_SIZE_M)\n",
    "t_impr = TriestImpr(MEMORY_SIZE_M)\n",
    "\n",
    "# Run the simulation\n",
    "total_edges = load_stream_and_run(FILE_PATH, t_base, t_impr)\n",
    "\n",
    "if total_edges is not None:\n",
    "    print(\"=\" * 40)\n",
    "    print(f\"Final Statistics:\")\n",
    "    # FIX: Access 't' via the reservoir object\n",
    "    print(f\"Total Edges Streamed (t): {t_base.reservoir.t}\")\n",
    "    print(f\"Reservoir Size (M):       {MEMORY_SIZE_M}\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    # Get Estimations\n",
    "    est_base = int(t_base.get_estimation())\n",
    "    est_impr = int(t_impr.get_estimation())\n",
    "    \n",
    "    print(f\"TRIEST-BASE Estimated Global Triangles: {est_base}\")\n",
    "    print(f\"TRIEST-IMPR Estimated Global Triangles: {est_impr}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e06cb2c3",
   "metadata": {},
   "source": [
    "## Algorithms Analysis and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f7fed22d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "ANALYSIS: data/web-Stanford.txt (M=10000)\n",
      "==================================================\n",
      "Ground Truth Triangles: 11,329,473\n",
      "--------------------------------------------------\n",
      "TRIÈST-BASE (Algorithm 1):\n",
      "  > Estimation:       171,734,121,754\n",
      "  > Absolute Error:   171,722,792,281\n",
      "  > Relative Error:   1515717.3885%\n",
      "  > Accuracy:         -1515617.3885%\n",
      "\n",
      "TRIÈST-IMPR (Algorithm 2):\n",
      "  > Estimation:       16,651,610\n",
      "  > Absolute Error:   5,322,137\n",
      "  > Relative Error:   46.9760%\n",
      "  > Accuracy:         53.0240%\n",
      "--------------------------------------------------\n",
      "Conclusion: TRIEST-IMPR provided a more accurate estimation.\n"
     ]
    }
   ],
   "source": [
    "ground_truth = 11329473  # From the dataset statistics\n",
    "\n",
    "# Calculate Metrics for TRIEST-BASE\n",
    "base_abs_error = abs(est_base - ground_truth)\n",
    "base_rel_error = (base_abs_error / ground_truth) * 100\n",
    "base_accuracy = 100.0 - base_rel_error\n",
    "\n",
    "# Calculate Metrics for TRIEST-IMPR\n",
    "impr_abs_error = abs(est_impr - ground_truth)\n",
    "impr_rel_error = (impr_abs_error / ground_truth) * 100\n",
    "impr_accuracy = 100.0 - impr_rel_error\n",
    "\n",
    "# Print Comparative Report\n",
    "\n",
    "print(\"=\" * 50)\n",
    "print(f\"ANALYSIS: {FILE_PATH} (M={MEMORY_SIZE_M})\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Ground Truth Triangles: {ground_truth:,}\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# TRIEST-BASE\n",
    "print(\"TRIÈST-BASE (Algorithm 1):\")\n",
    "print(f\"  > Estimation:       {est_base:,}\")\n",
    "print(f\"  > Absolute Error:   {base_abs_error:,}\")\n",
    "print(f\"  > Relative Error:   {base_rel_error:.4f}%\")\n",
    "print(f\"  > Accuracy:         {base_accuracy:.4f}%\")\n",
    "print(\"\")\n",
    "\n",
    "# TRIEST-IMPR\n",
    "print(\"TRIÈST-IMPR (Algorithm 2):\")\n",
    "print(f\"  > Estimation:       {est_impr:,}\")\n",
    "print(f\"  > Absolute Error:   {impr_abs_error:,}\")\n",
    "print(f\"  > Relative Error:   {impr_rel_error:.4f}%\")\n",
    "print(f\"  > Accuracy:         {impr_accuracy:.4f}%\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Concluding thought\n",
    "if impr_rel_error < base_rel_error:\n",
    "    print(\"Conclusion: TRIEST-IMPR provided a more accurate estimation.\")\n",
    "else:\n",
    "    print(\"Conclusion: TRIEST-BASE provided a (statistically unlikely) more accurate estimation.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33d2e7a3",
   "metadata": {},
   "source": [
    "## Questions\n",
    "\n",
    "### 1. What were the challenges you faced when implementing the algorithm?\n",
    "\n",
    "We faced several challenges, ranging from conceptual logic to subtle implementation bugs:\n",
    "\n",
    "1.  **Duplicate Edges and State Mismatch (`KeyError`):** The biggest challenge was a `KeyError` during the processing of the `web-Stanford.txt` dataset. This happened because the dataset contains reciprocal edges (e.g., $A \\to B$ and $B \\to A$). Our notebook canonicalized these to undirected edges ($A-B$), creating *duplicates* in the stream.\n",
    "    * **The Bug:** Our `ReservoirSampling` class used a `list` and correctly stored multiple copies of the same edge. However, our adjacency list (`self.adj`) used a `set`, which only stored one copy. When the *first* duplicate edge was evicted, it was correctly removed from `self.adj`. When the *second* duplicate edge was evicted later, the code tried to `.remove()` the edge from `self.adj` again, but it was already gone, causing a crash.\n",
    "    * **The Fix:** We replaced `.remove()` with `.discard()`, which safely does nothing if the item is already gone.\n",
    "\n",
    "2.  **Refactoring State (`AttributeError`):** A second challenge came after we refactored the code to create a separate `ReservoirSampling` class.\n",
    "    * **The Bug:** We moved the time-step variable `t` inside the `ReservoirSampling` object. The main notebook, however, tried to access it from the `TriestBase` object (`t_base.t`), causing an `AttributeError`.\n",
    "    * **The Fix:** We had to update the notebook to access the time-step via `t_base.reservoir.t`, correctly reflecting the new, modular structure.\n",
    "\n",
    "3.  **Python's Import System (`ModuleNotFoundError`):** When `TriestBase.py` (which is in the `src/` folder) tried to import `ReservoirSampling.py` (also in `src/`), it failed. This is because the main notebook was running from the parent directory, and Python's \"system path\" did not include the `src/` folder.\n",
    "    * **The Fix:** We had to add `sys.path.append(\"src\")` at the beginning of the notebook to tell Python to look inside the `src/` folder for modules.\n",
    "\n",
    "### 2. Can the algorithm be easily parallelized? If yes, how? If not, why? Explain.\n",
    "\n",
    "No, the algorithm is **not easily parallelized** because it is **inherently sequential**.\n",
    "\n",
    "The core logic of a streaming algorithm relies on its state at time $t$ being a function of its state at time $t-1$. This creates a strong dependency that resists parallel processing.\n",
    "\n",
    "* **Sequential Dependency:** The sampling decision for the $t$-th edge depends on the value $t$ (e.g., `random.random() < M/t`). If you split the stream into two chunks and process them on different cores, the second core would restart its $t$ from 1, making its probabilities incorrect (e.g., $M/1$ instead of $M/1000001$).\n",
    "* **State Dependency:** The contents of the reservoir $\\mathcal{S}$ at time $t$ depend on every single sampling decision made before it. The second core would have no knowledge of the reservoir $\\mathcal{S}$ built by the first core.\n",
    "* **Partial Parallelism:** While the *stream* processing is sequential, the *work* done for each edge *could* be partially parallelized. For example, the `get_common_neighbors(u, v)` function (which intersects two lists of neighbors) is the main bottleneck. This specific set intersection could be optimized or run on parallel hardware. However, the algorithm would still have to process the stream one edge at a time.\n",
    "\n",
    "### 3. Does the algorithm work for unbounded graph streams? Explain.\n",
    "\n",
    "**Yes, absolutely.** This is the primary strength and purpose of using Reservoir Sampling.\n",
    "\n",
    "The algorithm is designed to handle a stream of unknown (and potentially infinite) length with a *fixed* amount of memory $M$.\n",
    "\n",
    "* **Fixed Memory:** The algorithm's memory usage is $O(M)$, regardless of how many edges $t$ have been processed.\n",
    "* **Adapting Probability:** The sampling probability $p = M/t$ automatically adapts. As the stream grows (i.g., $t \\to \\infty$), the probability of sampling any *new* edge approaches zero, but it never *is* zero.\n",
    "* **Contrast with Fixed-Probability:** This is superior to the \"fixed-probability\" sampling (e.g., \"sample every edge with $p=0.01$\") mentioned in the paper. In that model, the sample size grows linearly with the stream ($0.01 \\times t$) and would eventually exhaust all memory on an unbounded stream. TRIÈST avoids this by fixing the memory $M$ and letting the probability $p$ adapt.\n",
    "\n",
    "### 4. Does the algorithm support edge deletions? If not, what modification would it need? Explain.\n",
    "\n",
    "The two algorithms we implemented, **TRIÈST-BASE** and **TRIÈST-IMPR**, **do not support edge deletions.**\n",
    "\n",
    "* **Why Not:** They are designed for \"insertion-only\" streams. Their entire mathematical framework (the scaling factor $\\xi^{(t)}$ and weight $\\eta^{(t)}$) is based on the assumption that the stream size $t$ is always growing. A deletion would break this math and the unbiased nature of the estimator.\n",
    "\n",
    "* **What Modification is Needed:** To support deletions, we would need to implement a completely different algorithm, which the paper provides in **Section 4.3: TRIÈST-FD (Fully-Dynamic)**.\n",
    "\n",
    "* **How TRIÈST-FD Works:**\n",
    "    1.  It replaces standard Reservoir Sampling with a more complex scheme called **Random Pairing (RP)**.\n",
    "    2.  It maintains special counters for \"uncompensated deletions.\" When an edge `(-, e)` arrives, if $e$ is in the sample, it's removed and a counter $d_i$ (deletion *in* sample) is incremented. If $e$ is *not* in the sample, a counter $d_o$ (deletion *out* of sample) is incremented.\n",
    "    3.  When a new edge `(+, e)` arrives, it doesn't just face the $M/t$ probability. It might instead be used to \"compensate\" for a past deletion by filling the \"hole\" in the sample, which uses a different probability calculation.\n",
    "\n",
    "In short: **No**, the versions we built cannot handle deletions. The paper *does* provide a solution, but it requires swapping out the entire sampling algorithm (Reservoir Sampling) for a more complex one (Random Pairing)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
