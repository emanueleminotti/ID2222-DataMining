{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Homework 3: Mining Data Streams\n",
    "\n",
    "This notebook implements and evaluates the **TRIÈST-BASE** and **TRIÈST-IMPR** algorithms as described in the paper \"TRIÈST: Counting Local and Global Triangles in Fully-Dynamic Streams with Fixed Memory Size.\"\n",
    "\n",
    "## Dataset: Stanford Web Graph (`web-Stanford.txt`)\n",
    "\n",
    "The experiment is run on the `web-Stanford` dataset.\n",
    "\n",
    "> Nodes represent pages from Stanford University (stanford.edu) and directed edges represent hyperlinks between them. The data was collected in 2002.\n",
    "\n",
    "**Note:** For this project, the directed edges are treated as undirected to form a graph for triangle counting.\n",
    "\n",
    "### Dataset Statistics\n",
    "\n",
    "| Statistic | Value |\n",
    "| :--- | :--- |\n",
    "| Nodes | 281,903 |\n",
    "| Edges | 2,312,497 |\n",
    "| Average clustering coefficient | 0.5976 |\n",
    "| **Number of triangles (Ground Truth)** | **11,329,473** |\n",
    "| Nodes in largest WCC | 255,265 (0.906) |\n",
    "| Edges in largest WCC | 2,234,572 (0.966) |"
   ],
   "id": "4d83dba4a5583e9e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Conceptual summary:\n",
    "\n",
    "**TRIEST-BASE** estimates the number of triangles in a graph stream using reservoir sampling to keep a fixed-size sample of edges.\n",
    "Each new edge is either stored (if the reservoir is not full) or kept with probability M/t and replaces a random sampled edge.\n",
    "\n",
    "Whenever an edge is added or removed from the sample, the algorithm updates triangle counts based on the common neighbors of its endpoints in the sampled graph.\n",
    "\n",
    "The final estimate is:\n",
    "\n",
    "* **Final Estimate = $\\tau \\times \\xi^{(t)}$**\n",
    "\n",
    "Where for $t > M$, the scaling factor is:\n",
    "\n",
    "$$\n",
    "\\xi^{(t)} = \\frac{t(t-1)(t-2)}{M(M-1)(M-2)}\n",
    "$$\n",
    "\n",
    "where $\\tau$ is the number of sampled triangles.\n",
    "\n",
    "Key idea: unbiased but unstable when the stream is large relative to M.\n",
    "\n",
    "\n",
    "**TRIEST-IMPR** improves accuracy by updating triangle counts before performing reservoir sampling and by weighting each update to account for sampling probability.\n",
    "\n",
    "For every incoming edge, the algorithm:\n",
    "   * 1.\tCounts common neighbors in the sampled graph.\n",
    "   * 2.\tAdds a weighted contribution \\\n",
    "$$\\eta = 1 \\quad \\text{(if } t \\le M),\n",
    "\\quad\\text{otherwise}\\quad\n",
    "\\eta = \\frac{(t-1)(t-2)}{M(M-1)}. $$\n",
    "   * 3.\tThen applies reservoir sampling, but does not decrement counters when edges are removed.\n",
    "\n",
    "Unlike TRIEST-BASE, the maintained counter is already the final estimate.\n",
    "\n",
    "Key idea: far more stable and accurate using the same memory.\n"
   ],
   "id": "70e63a53a4dfb68b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Imports and Setup",
   "id": "c03db581decfd73e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-22T11:55:53.258351Z",
     "start_time": "2025-11-22T11:55:53.254630Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import time\n",
    "import os\n",
    "import random\n",
    "# Import the algorithms from the local files\n",
    "from src.TriestBase import TriestBase\n",
    "from src.TriestImpr import TriestImpr\n",
    "\n",
    "\n",
    "# Configuration\n",
    "FILE_PATH = 'data/web-Stanford.txt'\n",
    "MEMORY_SIZE_M = 10000  # Fixed memory size M"
   ],
   "id": "d17c5e216d683f77",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Stream Processing Function",
   "id": "f0f8e40a85f0aa41"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-22T11:55:53.269070Z",
     "start_time": "2025-11-22T11:55:53.265676Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def load_stream_and_run(filepath, algo_base, algo_impr, limit=None):\n",
    "    \"\"\"\n",
    "    Reads the file stream and feeds edges to both algorithms simultaneously.\n",
    "    Handles the input as an edge stream (u, v).\n",
    "    \"\"\"\n",
    "    edge_count = 0\n",
    "    start_time = time.time()\n",
    "    \n",
    "    print(f\"Reading stream from {filepath}...\")\n",
    "    \n",
    "    try:\n",
    "        with open(filepath, 'r') as f:\n",
    "            for line in f:\n",
    "                # Skip comments\n",
    "                if line.startswith('#'):\n",
    "                    continue\n",
    "                \n",
    "                parts = line.split()\n",
    "                if len(parts) < 2:\n",
    "                    continue\n",
    "                \n",
    "                try:\n",
    "                    u, v = int(parts[0]), int(parts[1])\n",
    "                except ValueError:\n",
    "                    continue # Skip malformed lines\n",
    "                \n",
    "                # Ignore self-loops as per standard graph stream definitions\n",
    "                if u == v:\n",
    "                    continue\n",
    "                    \n",
    "                # Canonicalize edge (undirected graph assumption for TRIEST)\n",
    "                if u > v:\n",
    "                    u, v = v, u\n",
    "                \n",
    "                # Feed stream to both algorithms\n",
    "                algo_base.process_edge(u, v)\n",
    "                algo_impr.process_edge(u, v)\n",
    "                \n",
    "                edge_count += 1\n",
    "                if edge_count % 100000 == 0:\n",
    "                    print(f\"Processed {edge_count} edges...\")\n",
    "                \n",
    "                if limit and edge_count >= limit:\n",
    "                    break\n",
    "                    \n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: File {filepath} not found.\")\n",
    "        return None\n",
    "        \n",
    "    duration = time.time() - start_time\n",
    "    print(f\"\\n--- Processing Complete in {duration:.2f} seconds ---\")\n",
    "    return edge_count"
   ],
   "id": "7ab5e20fbc616225",
   "outputs": [],
   "execution_count": 13
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Execution and Results",
   "id": "af15967957ba8937"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-22T11:55:55.267058Z",
     "start_time": "2025-11-22T11:55:53.273858Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(f\"Initializing Algorithms with Memory M = {MEMORY_SIZE_M}\")\n",
    "t_base = TriestBase(MEMORY_SIZE_M)\n",
    "t_impr = TriestImpr(MEMORY_SIZE_M)\n",
    "\n",
    "# Run the simulation\n",
    "total_edges = load_stream_and_run(FILE_PATH, t_base, t_impr)\n",
    "\n",
    "if total_edges is not None:\n",
    "    print(\"=\" * 40)\n",
    "    print(f\"Final Statistics:\")\n",
    "    # FIX: Access 't' via the reservoir object\n",
    "    print(f\"Total Edges Streamed (t): {t_base.reservoir.t}\")\n",
    "    print(f\"Reservoir Size (M):       {MEMORY_SIZE_M}\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    # Get Estimations\n",
    "    est_base = int(t_base.get_estimation())\n",
    "    est_impr = int(t_impr.get_estimation())\n",
    "    \n",
    "    print(f\"TRIEST-BASE Estimated Global Triangles: {est_base}\")\n",
    "    print(f\"TRIEST-IMPR Estimated Global Triangles: {est_impr}\")"
   ],
   "id": "2a717d4f2c7fa49c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing Algorithms with Memory M = 10000\n",
      "Reading stream from data/web-Stanford.txt...\n",
      "Processed 100000 edges...\n",
      "Processed 200000 edges...\n",
      "Processed 300000 edges...\n",
      "Processed 400000 edges...\n",
      "Processed 500000 edges...\n",
      "Processed 600000 edges...\n",
      "Processed 700000 edges...\n",
      "Processed 800000 edges...\n",
      "Processed 900000 edges...\n",
      "Processed 1000000 edges...\n",
      "Processed 1100000 edges...\n",
      "Processed 1200000 edges...\n",
      "Processed 1300000 edges...\n",
      "Processed 1400000 edges...\n",
      "Processed 1500000 edges...\n",
      "Processed 1600000 edges...\n",
      "Processed 1700000 edges...\n",
      "Processed 1800000 edges...\n",
      "Processed 1900000 edges...\n",
      "Processed 2000000 edges...\n",
      "Processed 2100000 edges...\n",
      "Processed 2200000 edges...\n",
      "Processed 2300000 edges...\n",
      "\n",
      "--- Processing Complete in 1.99 seconds ---\n",
      "========================================\n",
      "Final Statistics:\n",
      "Total Edges Streamed (t): 2312497\n",
      "Reservoir Size (M):       10000\n",
      "----------------------------------------\n",
      "TRIEST-BASE Estimated Global Triangles: 176471870701\n",
      "TRIEST-IMPR Estimated Global Triangles: 17110074\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Algorithms Analysis and Evaluation",
   "id": "75e22f48e1f1dda4"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-22T11:55:55.276911Z",
     "start_time": "2025-11-22T11:55:55.270114Z"
    }
   },
   "cell_type": "code",
   "source": [
    "ground_truth = 11329473  # From the dataset statistics\n",
    "\n",
    "# Calculate Metrics for TRIEST-BASE\n",
    "base_abs_error = abs(est_base - ground_truth)\n",
    "base_rel_error = (base_abs_error / ground_truth) * 100\n",
    "base_accuracy = 100.0 - base_rel_error\n",
    "\n",
    "# Calculate Metrics for TRIEST-IMPR\n",
    "impr_abs_error = abs(est_impr - ground_truth)\n",
    "impr_rel_error = (impr_abs_error / ground_truth) * 100\n",
    "impr_accuracy = 100.0 - impr_rel_error\n",
    "\n",
    "# Print Comparative Report\n",
    "\n",
    "print(\"=\" * 50)\n",
    "print(f\"ANALYSIS: {FILE_PATH} (M={MEMORY_SIZE_M})\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Ground Truth Triangles: {ground_truth:,}\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# TRIEST-BASE\n",
    "print(\"TRIÈST-BASE (Algorithm 1):\")\n",
    "print(f\"  > Estimation:       {est_base:,}\")\n",
    "print(f\"  > Absolute Error:   {base_abs_error:,}\")\n",
    "print(f\"  > Relative Error:   {base_rel_error:.4f}%\")\n",
    "print(f\"  > Accuracy:         {base_accuracy:.4f}%\")\n",
    "print(\"\")\n",
    "\n",
    "# TRIEST-IMPR\n",
    "print(\"TRIÈST-IMPR (Algorithm 2):\")\n",
    "print(f\"  > Estimation:       {est_impr:,}\")\n",
    "print(f\"  > Absolute Error:   {impr_abs_error:,}\")\n",
    "print(f\"  > Relative Error:   {impr_rel_error:.4f}%\")\n",
    "print(f\"  > Accuracy:         {impr_accuracy:.4f}%\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Concluding thought\n",
    "if impr_rel_error < base_rel_error:\n",
    "    print(\"Conclusion: TRIEST-IMPR provided a more accurate estimation.\")\n",
    "else:\n",
    "    print(\"Conclusion: TRIEST-BASE provided a (statistically unlikely) more accurate estimation.\")"
   ],
   "id": "972e3fb280fdc7b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "ANALYSIS: data/web-Stanford.txt (M=10000)\n",
      "==================================================\n",
      "Ground Truth Triangles: 11,329,473\n",
      "--------------------------------------------------\n",
      "TRIÈST-BASE (Algorithm 1):\n",
      "  > Estimation:       176,471,870,701\n",
      "  > Absolute Error:   176,460,541,228\n",
      "  > Relative Error:   1557535.2996%\n",
      "  > Accuracy:         -1557435.2996%\n",
      "\n",
      "TRIÈST-IMPR (Algorithm 2):\n",
      "  > Estimation:       17,110,074\n",
      "  > Absolute Error:   5,780,601\n",
      "  > Relative Error:   51.0227%\n",
      "  > Accuracy:         48.9773%\n",
      "--------------------------------------------------\n",
      "Conclusion: TRIEST-IMPR provided a more accurate estimation.\n"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Questions\n",
    "\n",
    "### 1. What were the challenges you faced when implementing the algorithm?\n",
    "\n",
    "The main challenge we faced was related to duplicate edges and state mismatch.\n",
    "The  challenge was a `KeyError` during the processing of the `web-Stanford.txt` dataset. This happened because the dataset contains reciprocal edges (e.g., $A \\to B$ and $B \\to A$). Our notebook canonicalized these to undirected edges ($A-B$), creating *duplicates* in the stream.\n",
    "   * **The Bug:** Our `ReservoirSampling` class used a `list` and correctly stored multiple copies of the same edge. However, our adjacency list (`self.adj`) used a `set`, which only stored one copy. When the *first* duplicate edge was evicted, it was correctly removed from `self.adj`. When the *second* duplicate edge was evicted later, the code tried to `.remove()` the edge from `self.adj` again, but it was already gone, causing a crash.\n",
    "   * **The Fix:** We replaced `.remove()` with `.discard()`, which safely does nothing if the item is already gone.\n",
    "\n",
    "\n",
    "### 2. Can the algorithm be easily parallelized? If yes, how? If not, why? Explain.\n",
    "\n",
    "No, the algorithm is not easily parallelized because it is inherently sequential.\n",
    "\n",
    "The core logic of a streaming algorithm relies on its state at time $t$ being a function of its state at time $t-1$. This creates a strong dependency that resists parallel processing.\n",
    "\n",
    "* **Sequential Dependency:** The sampling decision for the $t$-th edge depends on the value $t$ (e.g., `random.random() < M/t`). If you split the stream into two chunks and process them on different cores, the second core would restart its $t$ from 1, making its probabilities incorrect (e.g., $M/1$ instead of $M/1000001$).\n",
    "* **State Dependency:** The contents of the reservoir $\\mathcal{S}$ at time $t$ depend on every single sampling decision made before it. The second core would have no knowledge of the reservoir $\\mathcal{S}$ built by the first core.\n",
    "* **Partial Parallelism:** While the *stream* processing is sequential, the *work* done for each edge *could* be partially parallelized. For example, the `get_common_neighbors(u, v)` function (which intersects two lists of neighbors) is the main bottleneck. This specific set intersection could be optimized or run on parallel hardware. However, the algorithm would still have to process the stream one edge at a time.\n",
    "\n",
    "### 3. Does the algorithm work for unbounded graph streams? Explain.\n",
    "\n",
    "Yes, absolutely. This is the primary strength and purpose of using Reservoir Sampling.\n",
    "\n",
    "The algorithm is designed to handle a stream of unknown (and potentially infinite) length with a *fixed* amount of memory $M$.\n",
    "\n",
    "* **Fixed Memory:** The algorithm's memory usage is $O(M)$, regardless of how many edges $t$ have been processed.\n",
    "* **Adapting Probability:** The sampling probability $p = M/t$ automatically adapts. As the stream grows (i.g., $t \\to \\infty$), the probability of sampling any *new* edge approaches zero, but it never *is* zero.\n",
    "* **Contrast with Fixed-Probability:** This is superior to the \"fixed-probability\" sampling (e.g., \"sample every edge with $p=0.01$\") mentioned in the paper. In that model, the sample size grows linearly with the stream ($0.01 \\times t$) and would eventually exhaust all memory on an unbounded stream. TRIÈST avoids this by fixing the memory $M$ and letting the probability $p$ adapt.\n",
    "\n",
    "### 4. Does the algorithm support edge deletions? If not, what modification would it need? Explain.\n",
    "\n",
    "The two algorithms we implemented, TRIÈST-BASE and TRIÈST-IMPR, do not support edge deletions.\n",
    "\n",
    "* **Why Not:** They are designed for \"insertion-only\" streams. Their entire mathematical framework (the scaling factor $\\xi^{(t)}$ and weight $\\eta^{(t)}$) is based on the assumption that the stream size $t$ is always growing. A deletion would break this math and the unbiased nature of the estimator.\n",
    "\n",
    "* **What Modification is Needed:** To support deletions, we would need to implement a completely different algorithm, which the paper provides in **Section 4.3: TRIÈST-FD (Fully-Dynamic)**.\n",
    "\n",
    "* **How TRIÈST-FD Works:**\n",
    "    1.  It replaces standard Reservoir Sampling with a more complex scheme called **Random Pairing (RP)**.\n",
    "    2.  It maintains special counters for \"uncompensated deletions.\" When an edge `(-, e)` arrives, if $e$ is in the sample, it's removed and a counter $d_i$ (deletion *in* sample) is incremented. If $e$ is *not* in the sample, a counter $d_o$ (deletion *out* of sample) is incremented.\n",
    "    3.  When a new edge `(+, e)` arrives, it doesn't just face the $M/t$ probability. It might instead be used to \"compensate\" for a past deletion by filling the \"hole\" in the sample, which uses a different probability calculation.\n",
    "\n",
    "In short: No, the versions we built cannot handle deletions. The paper *does* provide a solution, but it requires swapping out the entire sampling algorithm (Reservoir Sampling) for a more complex one (Random Pairing)."
   ],
   "id": "a22ca26ae9713aa0"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
